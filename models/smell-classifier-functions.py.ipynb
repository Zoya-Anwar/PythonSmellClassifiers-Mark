{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a489f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "!pip install bayesian-optimization\n",
    "\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier, cv\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcffa94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set SEED to define random state of any randomised functions\n",
    "SEED=0\n",
    "# Use seaborn theme \n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ff670",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4487207",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_dragon():\n",
    "    # load dragon descriptors\n",
    "    dataset='Dragon'\n",
    "    data_path = './data/alvaDescDescriptors.txt'\n",
    "    na_values=['na']\n",
    "    df = pd.read_csv(data_path, sep=\"\\t\",  dtype={'pctapi': np.float64}, na_values=na_values)\n",
    "    smiles =  np.loadtxt(\"./data/smiles.txt\", dtype='str')\n",
    "    df = df.drop(['No.', 'NAME'], axis=1)\n",
    "    df = df.set_index(smiles)\n",
    "    \n",
    "    # extract odor classes from the Modred dataset\n",
    "    data_path = './data/master_4Mayhew.xlsx'\n",
    "    df_mor, _, _ = load_dataset_modred()\n",
    "    y=df_mor['label'] # labels, independent variables\n",
    "    X=df # features, dependent variables\n",
    "    X.columns = X.columns.str.translate(\"\".maketrans({\"[\":\"{\", \"]\":\"}\",\"<\":\"^\"}))\n",
    "\n",
    "    return df, X, y\n",
    "\n",
    "def load_dataset_modred():\n",
    "    dataset='Modred'\n",
    "    data_path = './data/master_4Mayhew.xlsx'\n",
    "    df = pd.read_excel(data_path)\n",
    "    df.set_index('SMILES', inplace=True)\n",
    "    y=df['label'] # labels, independent variables\n",
    "    X=df.drop(['label'], axis=1) # features, dependent variables\n",
    "\n",
    "    return df, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f003ca",
   "metadata": {},
   "source": [
    "## Initial stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_df_stats(df):\n",
    "    print(f\"Number of columns with all NaNs \\n{(df.isna().mean(axis=0) == 1).value_counts()}\")\n",
    "    print(f\"\\n Number of columns with >0.9 NaNs \\n{(df.isna().mean(axis=0) > 0.9).value_counts()}\")\n",
    "    print(f\"\\n Number of columns with a NaN \\n{ len(df.columns[df.isna().any()].tolist())}\")\n",
    "    \n",
    "    percs = df.isna().mean(axis=0)\n",
    "    ax = percs[(percs > 0)].plot.hist(bins=15)\n",
    "    plt.xlabel(\"Percentage NaNs in descriptors with missing values\")\n",
    "    plt.show()\n",
    "    \n",
    "    df.describe()\n",
    "    \n",
    "    corrs = df.corr()\n",
    "    corrs.to_csv(f'results/correlations/descriptor-correlations-{dataset}.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrs_important_features(model, X_test, model_name, dataset, nlargest=100, heatmap=14):\n",
    "    feature_imp = pd.Series(model.feature_importances_,index=X_test.columns.values).sort_values(ascending=False)\n",
    "    top_100 = feature_imp.nlargest(100)\n",
    "    important_df = df.loc[:,top_100.index]\n",
    "    corrs = important_df.corr()\n",
    "    \n",
    "    print(f\"Extracted correlation scores for most important {nlargest} features for {model_name}\")\n",
    "    corrs.to_csv(f'results/correlations/descriptor-corrs-top{nlargest}-{model_name}-{dataset}.csv')  \n",
    "    \n",
    "    # Print heatmap for top n\n",
    "    print(f\"Printing heatmap for most important {heatmap} features\")\n",
    "    \n",
    "    top = feature_imp.nlargest(heatmap)\n",
    "    important_df = df.loc[:,top.index]\n",
    "    corrs = important_df.corr()\n",
    "    \n",
    "    plt.figure(figsize = (16,16))\n",
    "    heatmap = sns.heatmap(corrs_10, vmin=-1, vmax=1, annot=True)\n",
    "    # Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\n",
    "    heatmap.set_title(f'Correlation Heatmap {dataset} - top {heatmap} descriptors', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71a5ea",
   "metadata": {},
   "source": [
    "## Splitting the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0) # 70% training and 30% test\n",
    "    percentage_odorless = y_train.value_counts()[False]/y_train.shape[0]\n",
    "    percentage_odor = 1 - percentage_odorless\n",
    "\n",
    "    print(f\"Total number molecules in training set: {y_train.shape[0]}\")\n",
    "    print(f\"Odor: {y_train.value_counts()[True]}\")\n",
    "    print(f\"Odorless: {y_train.value_counts()[False]}\")\n",
    "\n",
    "    print(f\"\\nTotal number molecules in test set: {y_test.shape[0]}\")\n",
    "    print(f\"Odor: {y_test.value_counts()[True]}\")\n",
    "    print(f\"Odorless: {y_test.value_counts()[False]}\")\n",
    "\n",
    "    print(f\"\\nPercentage odorless {y_test.value_counts()[False]/y_test.shape[0]}\")\n",
    "    \n",
    "    X_train_t, X_val, y_train_t, y_val = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train, random_state=SEED)\n",
    "\n",
    "    print(\"Number of training samples:\", len(X_train_t))\n",
    "    print(\"Number of validation samples:\", len(X_val))\n",
    "    \n",
    "    return X_train, X_test, X_train_t, X_val, y_train, y_test, y_train_t, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e52b016",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32141f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "\n",
    "# Scale dataset \n",
    "\n",
    "def preprocessing_df(df): \n",
    "    # Correlation\n",
    "    corrs = df.corr()\n",
    "    # select upper traingle of correlation matrix\n",
    "    upper = corrs.where(np.triu(np.ones(corrs.shape),k=1).astype(bool))\n",
    "    # Find index of columns with correlation greater than 0.99\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.99)]\n",
    "    # drop the columns\n",
    "    df.drop(to_drop, axis=1, inplace=True)\n",
    "    # Remove descriptors with all NaNs \n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "def scale_dataset(X_train, X_test, X_train_t, X_val):\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train_transformed = min_max_scaler.fit_transform(X_train)\n",
    "\n",
    "    X_train =  pd.DataFrame(X_train_transformed, columns=X_train.columns[0:], index=X_train.index)\n",
    "    X_test = pd.DataFrame(min_max_scaler.transform(X_test), columns=X_test.columns[0:], index=X_test.index)\n",
    "\n",
    "    min_max_scaler2 = preprocessing.MinMaxScaler()\n",
    "    X_train_t_transformed = min_max_scaler2.fit_transform(X_train_t)\n",
    "\n",
    "    X_train_t = pd.DataFrame(X_train_t_transformed, columns=X_train_t.columns[0:], index=X_train_t.index)\n",
    "    X_val = pd.DataFrame(min_max_scaler2.transform(X_val), columns=X_val.columns[0:], index=X_val.index)\n",
    "\n",
    "    return X_train, X_test, X_train_t, X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d012fb7",
   "metadata": {},
   "source": [
    "# SKLEARN helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b52d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, y_test, X_test, verbose=True):\n",
    "    y_pred=model.predict(X_test)\n",
    "    y_pred_probs=model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"ROC_AUC (TEST):\",metrics.roc_auc_score(y_test, y_pred_probs))\n",
    "        print(\"\\n\\nCLASSIFICATION REPORT:\\n\",metrics.classification_report(y_test, y_pred,  digits=4))\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_') and hasattr(X_test, 'columns'):\n",
    "            feature_imp = pd.Series(model.feature_importances_,index=X_test.columns.values).sort_values(ascending=False)\n",
    "            top_20 = feature_imp.nlargest(20)\n",
    "            sns.barplot(x=top_20, y=top_20.index)\n",
    "            plt.xlabel('Descriptor Importance Score')\n",
    "            plt.ylabel('Descriptor')\n",
    "            plt.title(\"Top descriptors\")\n",
    "            plt.show()\n",
    "        \n",
    "        fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_probs)  \n",
    "        \n",
    "        #create ROC curve\n",
    "        plt.plot(fpr,tpr)\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.show()\n",
    "        \n",
    "    return metrics.roc_auc_score(y_test, y_pred_probs)\n",
    "\n",
    "def test_model_cross_val(model, y_test, X_test, k=5, verbose=True):\n",
    "    roc_auc_nans = cross_val_score(model, X_test, y_test, cv=k, scoring='roc_auc')\n",
    "    print(f\"AVERAGE CV={k} ROC_AUC: {np.mean(roc_auc_nans)}\")\n",
    "    print(f\"AVERAGE CV={k} STD: {np.std(roc_auc_nans)}\")\n",
    "    return np.mean(roc_auc_nans), np.std(roc_auc_nans)\n",
    "\n",
    "def handle_importance(model, X_train, X_test=pd.DataFrame(), threshold=-1, n=-1, verbose=None):\n",
    "    feature_imp = pd.Series(model.feature_importances_,index=X_train.columns.values).sort_values(ascending=False)\n",
    "    if threshold >= 0:\n",
    "        feature_imp = feature_imp[feature_imp > threshold]\n",
    "    \n",
    "    if n >= 0: \n",
    "        feature_imp = feature_imp.nlargest(n)\n",
    "        \n",
    "    if verbose: \n",
    "        print(feature_imp)\n",
    "        \n",
    "    X_train = X_train.loc[:, feature_imp.axes[0].tolist()]\n",
    "    \n",
    "    if not X_test.empty:\n",
    "        X_test = X_test.loc[:, feature_imp.axes[0].tolist()]\n",
    "\n",
    "    return X_train, X_test, model \n",
    "\n",
    "def optimiseImportance(model, X_tr, y_tr, k=5, model_name=\"RF\", verbose=True):\n",
    "    \n",
    "    imp_params = [\n",
    "              {\"threshold\":-1, 'n':1},\n",
    "              {\"threshold\":-1, 'n':2},\n",
    "              {\"threshold\":-1, 'n':5}, \n",
    "              {\"threshold\":-1, 'n':10}, \n",
    "              {\"threshold\":-1, 'n':25}, \n",
    "              {\"threshold\":-1, 'n':50},\n",
    "              {\"threshold\":-1, 'n':100}, \n",
    "              {\"threshold\":-1, 'n':200}, \n",
    "              {\"threshold\":-1, 'n':400},\n",
    "              {\"threshold\":-1, 'n':X_tr.shape[1]}]\n",
    "    \n",
    "    sns.set(rc={'figure.figsize':(11.7,12.27)})\n",
    "    feature_imp = pd.Series(model.feature_importances_,index=X_tr.columns.values).sort_values(ascending=False)\n",
    "    top_50 = feature_imp.nlargest(50)\n",
    "    sns.barplot(x=top_50, y=top_50.index)\n",
    "    plt.xlabel('Descriptor Importance Score')\n",
    "    plt.ylabel('Descriptor')\n",
    "    plt.title(\"Top 50 descriptors\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Cross validated importance experiment with k={k}\")\n",
    "    \n",
    "    imp_df = pd.DataFrame(columns=['Most important N descriptors', 'Average AUROC', 'Standard Deviation'])\n",
    "\n",
    "    for imp_param in imp_params: \n",
    "        name = (f\"Threshold:{imp_param['threshold']}N:{imp_param['n']}\".replace(\"N:-1\", '')).replace(\"Threshold:-1\", '')\n",
    "        \n",
    "        X_tr_temp , _, _ = handle_importance(model, \n",
    "                                            X_tr.copy(), \n",
    "                                            threshold=imp_param['threshold'], \n",
    "                                            n=imp_param['n'])\n",
    "                \n",
    "        roc_auc_nans = cross_val_score(model, X_tr_temp, y_tr, cv=k, scoring='roc_auc')\n",
    "        \n",
    "        imp_df = imp_df.append({'Most important N descriptors': imp_param['n'], \n",
    "                               'Average AUROC':np.mean(roc_auc_nans), \n",
    "                               'Standard Deviation':np.std(roc_auc_nans)}, \n",
    "                                ignore_index = True)\n",
    "\n",
    "    if verbose:\n",
    "        fig = imp_df.plot(kind=\"bar\", x=\"Most important N descriptors\", y=\"Average AUROC\", \n",
    "                          yerr=\"Standard Deviation\", \n",
    "                          figsize=(8,5),\n",
    "                          title=f\"AUROC after filtering descriptors by importance CV={k} - {model_name} - {dataset}\", legend=False)\n",
    "        plt.ylim(0.80,1)\n",
    "    return imp_df\n",
    "\n",
    "def testWithoutImportantDescs(model, X_tr, y_tr, k=5, model_name=\"RF\", verbose=True):\n",
    "    imp_params = [\n",
    "                  {\"threshold\":-1, 'n':5}, \n",
    "                  {\"threshold\":-1, 'n':10}, \n",
    "                  {\"threshold\":-1, 'n':25}, \n",
    "                  {\"threshold\":-1, 'n':50},\n",
    "                  {\"threshold\":-1, 'n':100}, \n",
    "                  {\"threshold\":-1, 'n':200}, \n",
    "                  {\"threshold\":-1, 'n':400},\n",
    "                  {\"threshold\":-1, 'n':600}]\n",
    "    \n",
    "    imp_df = pd.DataFrame(columns=['Most important N descriptors', 'Average AUROC', 'Standard Deviation'])\n",
    "\n",
    "    roc_auc, std = test_model_cross_val(model, y_tr, X_tr)\n",
    "    imp_df = imp_df.append({'Most important N descriptors': 0, \n",
    "                       'Average AUROC':roc_auc, \n",
    "                       'Standard Deviation':std}, \n",
    "                        ignore_index = True)\n",
    "\n",
    "    \n",
    "    for imp_param in imp_params: \n",
    "        name = (f\"Threshold:{imp_param['threshold']}\")\n",
    "\n",
    "        feature_imp = pd.Series(model.feature_importances_,index=X_tr.columns.values).sort_values(ascending=False)\n",
    "        feature_imp = feature_imp.nlargest(imp_param['n'])\n",
    "        X_tr_temp = X_tr.drop(feature_imp.axes[0].tolist(), axis=1) \n",
    "        roc_auc_nans = cross_val_score(model, X_tr_temp, y_tr, cv=k, scoring='roc_auc')\n",
    "        \n",
    "        imp_df = imp_df.append({'Most important N descriptors': imp_param['n'], \n",
    "                               'Average AUROC':np.mean(roc_auc_nans), \n",
    "                               'Standard Deviation':np.std(roc_auc_nans)}, \n",
    "                                ignore_index = True)\n",
    "\n",
    "    if verbose:\n",
    "        fig = imp_df.plot(kind=\"bar\",figsize=(8,5), x=\"Most important N descriptors\", y=\"Average AUROC\",  ylabel=\"Average AUROC\",  yerr=\"Standard Deviation\", title=f\"AUROC after removing descriptors by importance CV={k}  - {model_name} - {dataset}\", legend=False)\n",
    "        plt.ylim(0.80,1)\n",
    "        \n",
    "    return imp_df\n",
    "\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def handleNaNs(X_tr, X_te, option=1, thresh=0.6): \n",
    "    if option == 1:\n",
    "        nans =  X.columns[X.isna().any()].tolist()\n",
    "        X_tr.drop(nans, inplace = True, axis=1)\n",
    "        X_te.drop(nans, inplace = True, axis=1)\n",
    "    else: \n",
    "        nans = X.isna().mean(axis=0)\n",
    "        # set threshold for percentage nans before we drop \n",
    "        X_thresh = nans[nans >= thresh]\n",
    "        X_tr.drop(X_thresh.index, \n",
    "          axis=1, \n",
    "          inplace=True)\n",
    "        X_te.drop(X_thresh.index, \n",
    "          axis=1, \n",
    "          inplace=True)\n",
    "        \n",
    "        if option == 3: \n",
    "            imputer = KNNImputer(n_neighbors=2)\n",
    "            X_tr_temp = imputer.fit_transform(X_tr)\n",
    "            X_te_temp = imputer.transform(X_te)\n",
    "            \n",
    "            X_tr =  pd.DataFrame(X_tr_temp, columns=X_tr.columns[0:], index=X_tr.index)\n",
    "            X_te = pd.DataFrame(X_te_temp, columns=X_te.columns[0:], index=X_te.index)\n",
    "    \n",
    "    return X_tr, X_te\n",
    "\n",
    "\n",
    "def optimiseNaNs(model, X_train, X_test, y_train, y_test, model_name=\"RF\", option=2, k=5, verbose=True): \n",
    "    print(f\"Cross validated missing values experiment with k={k}\")\n",
    "    \n",
    "    percentages = [x/100 for x in range(0, 100, 10)] \n",
    "    nans_df = pd.DataFrame(columns=['Percentage', 'Average AUROC', 'Standard Deviation'])\n",
    "\n",
    "    for perc in percentages: \n",
    "        name = str(perc)\n",
    "        if perc == 0:\n",
    "            X_train_cv, _  = handleNaNs(X_train.copy(), X_test.copy(), option=1)\n",
    "        else:\n",
    "            X_train_cv, _  = handleNaNs(X_train.copy(), X_test.copy(), option=option, thresh=perc)\n",
    "\n",
    "        roc_auc_nans = cross_val_score(model, X_train_cv, y_train, cv=k, scoring='roc_auc')\n",
    "        \n",
    "        nans_df = nans_df.append({'Percentage': name, \n",
    "                                   'Average AUROC':np.mean(roc_auc_nans), \n",
    "                                   'Standard Deviation':np.std(roc_auc_nans)}, \n",
    "                                    ignore_index = True)\n",
    "\n",
    "    if verbose:\n",
    "        fig = nans_df.plot(kind=\"bar\", x=\"Percentage\", y=\"Average AUROC\", ylabel=\"Average AUROC\", \n",
    "                           yerr=\"Standard Deviation\", title=f\"AUROC after thresholding descriptors by % NaNs CV={k} - {model_name} - {dataset}\", \n",
    "                           legend=False)\n",
    "        plt.ylim(0.9,1)\n",
    "        \n",
    "    return nans_df\n",
    "\n",
    "\n",
    "# Using PCA to reduce dimensionality \n",
    "\n",
    "from sklearn import decomposition \n",
    "\n",
    "def optimiseDimensions(clf, X_train, y_train): \n",
    "    pca_df = pd.DataFrame(columns=['Dimensions', 'Average AUROC', 'Standard Deviation'])\n",
    "\n",
    "    roc_aucs = []\n",
    "    stds = []\n",
    "    pcas = [1, 2, 5, 10, 20, 50, 100]\n",
    "\n",
    "    for n in pcas:\n",
    "        pca = decomposition.PCA(n_components=n)\n",
    "        pca_result = pca.fit_transform(X_train)\n",
    "        clf.fit(pca_result, y_train)\n",
    "\n",
    "        print(\"Validation performance of Random Forest after reducing dimensionality with PCA\")\n",
    "        roc_auc, std = test_model_cross_val(clf, y_train, pca_result, k=5, verbose=True)\n",
    "\n",
    "        pca_df = pca_df.append({'Dimensions': n, \n",
    "                               'Average AUROC':roc_auc, \n",
    "                               'Standard Deviation':std}, \n",
    "                                ignore_index = True)\n",
    "\n",
    "    clf.fit(X_train_rf, y_train_rf)\n",
    "    print(\"Validation performance of Random Forest with all dimensions\")\n",
    "    roc_auc, std = test_model_cross_val(clf, y_train, X_train, k=5, verbose=True)\n",
    "    roc_aucs.append(roc_auc)\n",
    "    pca_df = pca_df.append({'Dimensions': X_train.shape[-1], \n",
    "                           'Average AUROC':roc_auc, \n",
    "                           'Standard Deviation':std}, \n",
    "                            ignore_index = True)\n",
    "\n",
    "    fig = pca_df.plot(kind=\"bar\", x=\"Dimensions\", y=\"Average AUROC\", ylabel=\"Average AUROC\", \n",
    "                       yerr=\"Standard Deviation\", title=f\"AUROC after reducing dimensions using PCA CV=5 - RF - {dataset}\", \n",
    "                       legend=False, figsize=(10, 10))\n",
    "\n",
    "    plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600fa772",
   "metadata": {},
   "source": [
    "# Quick tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForestTuned(X_train, X_test, y_train, y_test): \n",
    "    # Run me if you want the tuned model without running training\n",
    "    clf=RandomForestClassifier(random_state=SEED) \n",
    "\n",
    "    # For readability\n",
    "    y_train_rf = y_train.copy()\n",
    "    y_test_rf = y_test.copy()\n",
    "    X_train_rf = X_train.copy()\n",
    "    X_test_rf = X_test.copy()\n",
    "\n",
    "    # Handle NaNs\n",
    "    X_train_rf, X_test_rf = handleNaNs(X_train.copy(), X_test.copy(), option=1)\n",
    "    X_train_t_rf, X_val_rf = handleNaNs(X_train_t.copy(), X_val.copy(), option=1)\n",
    "\n",
    "    # Fit model \n",
    "    clf.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "    # Handle features \n",
    "    X_train_rf, X_test_rf, clf = handle_importance(clf, \n",
    "                                                   X_train_rf, \n",
    "                                                   X_test_rf, \n",
    "                                                   threshold=-1, \n",
    "                                                   n=100)\n",
    "\n",
    "    # Create tuned model\n",
    "    params_tuned_rf_saved = {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 1, 'n_estimators': 199}\n",
    "    tuned_rf = RandomForestClassifier(**params_tuned_rf_saved, random_state=SEED)\n",
    "    tuned_rf.fit(X_train_rf, y_train_rf)\n",
    "    \n",
    "    return tuned_rf, X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def xgboostTuned(X_train, X_test, y_train, y_test): \n",
    "    # Run me if you want the tuned model without running training\n",
    "\n",
    "    # for readibility \n",
    "    y_train_xgb = y_train.copy()\n",
    "    y_test_xgb = y_test.copy()\n",
    "    X_train_xgb = X_train.copy()\n",
    "    X_test_xgb = X_test.copy()\n",
    "\n",
    "    xgb = XGBClassifier(random_state=SEED)\n",
    "    xgb.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "    # Handle NaNs\n",
    "    X_train_xgb, X_test_xgb = handleNaNs(X_train.copy(), X_test.copy(), option=2, thresh=0.1)\n",
    "    X_train_t_xgb, X_val_xgb = handleNaNs(X_train_t.copy(), X_val.copy(), option=2, thresh=0.1)\n",
    "\n",
    "    # Fit model \n",
    "    xgb.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "    # Handle features \n",
    "    X_train_xgb, X_test_xgb, xgb = handle_importance(xgb, \n",
    "                                                    X_train_xgb, \n",
    "                                                    X_test=X_test_xgb, \n",
    "                                                    threshold=-1, \n",
    "                                                    n=400)\n",
    "\n",
    "    X_train_t_xgb, X_val_xgb, xgb = handle_importance(xgb, \n",
    "                                                    X_train_t_xgb, \n",
    "                                                    X_test=X_val_xgb, \n",
    "                                                    threshold=-1, \n",
    "                                                    n=400)\n",
    "    xgb.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "    # Create tuned model\n",
    "    params_tuned_xgb_saved = {'colsample_bytree': 0.7, 'gamma': 0.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 564, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'scale_pos_weight': 2.5}\n",
    "    tuned_xgb.fit(X_train_xgb, y_train_xgb)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465f3b43",
   "metadata": {},
   "source": [
    "## Neural Network Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_nn(X_train_nn, X_test_nn, X_train_t_nn, X_val_nn, y_train_nn, y_test_nn, y_train_t_nn, y_val_nn, ): \n",
    "    # - You can't have missing values in a Neural Network, so we choose to use the optimal value for XGB\n",
    "    X_train_nn, X_test_nn = handleNaNs(X_train_nn.copy(), X_test_nn.copy(), option=3, thresh=0.1)\n",
    "    X_train_t_nn, X_val_nn = handleNaNs(X_train_t_nn.copy(), X_val_nn.copy(), option=3, thresh=0.1)\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train_t_nn)\n",
    "    \n",
    "    y_train_nn = encoder.transform(y_train_nn)\n",
    "    y_train_t_nn = encoder.transform(y_train_t_nn)\n",
    "    y_test_nn = encoder.transform(y_test_nn)\n",
    "    y_val_nn = encoder.transform(y_val_nn)\n",
    "\n",
    "def test_model_keras_train(model, history, X_val_nn, y_val_nn, X_train_nn, y_train_nn): \n",
    "    # Plot training and validation auc \n",
    "\n",
    "    plt.plot(history.history['auc'])\n",
    "    plt.plot(history.history['val_auc'])\n",
    "    plt.title('model roc_auc')\n",
    "    plt.ylabel('ROC_AUC')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation ROC curve\n",
    "    y_pred_keras = model.predict(X_val_nn).ravel()\n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_val_nn, y_pred_keras)\n",
    "    auc_v_keras = metrics.roc_auc_score(y_val_nn, y_pred_keras)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_keras, tpr_keras)\n",
    "\n",
    "    # Use the Keras model to make predictions on a test dataset\n",
    "    y_pred_t_keras = model.predict(X_train_nn)\n",
    "    \n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_train_nn, y_pred_t_keras)\n",
    "    auc_t_keras = metrics.roc_auc_score(y_train_nn, y_pred_t_keras)\n",
    "    \n",
    "    plt.plot(fpr_keras, tpr_keras)\n",
    "\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(['', 'Validation (area = {:.4f})'.format(auc_v_keras), 'Training (area = {:.3f})'.format(auc_t_keras)], loc='best')\n",
    "    plt.show()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report \n",
    "    y_pred = model.predict(X_val_nn).round()\n",
    "    print(metrics.classification_report(y_val_nn, y_pred, digits=4))\n",
    "    \n",
    "def test_model_keras(model, X_test_nn, y_test_nn, X_train_nn, y_train_nn):\n",
    "    y_pred_t_keras = model.predict(X_test_nn).ravel()\n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test_nn, y_pred_t_keras)\n",
    "    auc_t_keras = metrics.roc_auc_score(y_test_nn, y_pred_t_keras)\n",
    "    auc_t_keras\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_keras, tpr_keras)\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(['', 'Test (area = {:.4f})'.format(auc_t_keras)], loc='best')\n",
    "    plt.show()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report \n",
    "    y_pred = model.predict(X_test_nn).round()\n",
    "    print(metrics.classification_report(y_test_nn, y_pred, digits=4))\n",
    "    \n",
    "\n",
    "def model_builder_2(hp):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(Dense((X_train_t_nn_temp.shape[-1]), input_shape=((X_train_t_nn_temp.shape[-1]),), activation='relu'))\n",
    "    \n",
    "    # Tune the number of units in the first Dense layer\n",
    "    # Choose an optimal value between 2-length of layers\n",
    "    hp_units = hp.Int('unit_1', min_value=2, max_value=2000, step=10)\n",
    "    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    \n",
    "    # dropout layer reduces overfitting\n",
    "    hp_units = hp.Float('dropout_1', min_value=0.0, max_value=0.5, default=0.25, step=0.05)\n",
    "    model.add(keras.layers.Dropout(hp_units))\n",
    "    \n",
    "    # Tune the number of units in the second Dense layer\n",
    "    # Choose an optimal value between 2-length of layers\n",
    "    hp_units = hp.Int('unit_2', min_value=2, max_value=2000, step=10)\n",
    "    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 5e-2, 1e-3, 5e-3, 1e-4, 5e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=METRICS)\n",
    "\n",
    "    return model\n",
    "    \n",
    "\n",
    "import sklearn.feature_selection as fs\n",
    "from sklearn.svm import LinearSVC\n",
    "import random\n",
    "\n",
    "def feature_ranking_experiment(X_train_t_nn,X_val_nn, y_train_t_nn, y_val_nn ): \n",
    "    ranking = []\n",
    "    algos = ['random', 'rf', 'f_classif', 'mutual_info_classif']\n",
    "    ks = [5, 50, 100, 400]\n",
    "\n",
    "    accuracy_2 = pd.DataFrame(index = ks, columns = algos)\n",
    "\n",
    "    for algo in algos:\n",
    "          for k in ks:\n",
    "                X_train_t_nn_temp = X_train_t_nn.copy()\n",
    "                X_val_nn_temp = X_val_nn.copy()\n",
    "\n",
    "                if algo == 'random':\n",
    "                    indexes = list(range(X_train_t_nn.shape[-1]))\n",
    "                    random.shuffle(indexes)\n",
    "                    top_n = indexes[0:k]\n",
    "                    X_train_t_nn_temp = X_train_t_nn.iloc[:,top_n]\n",
    "                    X_val_nn_temp = X_val_nn.iloc[:,top_n]\n",
    "                else:\n",
    "                    if algo == 'f_classif': \n",
    "                        bk = fs.SelectKBest(fs.f_classif, k=k)\n",
    "                        bk.fit(X_train_t_nn, y_train_t_nn)\n",
    "                    elif algo == \"mutual_info_classif\":\n",
    "                        bk = fs.SelectKBest(fs.mutual_info_classif, k=k)\n",
    "                        bk.fit(X_train_t_nn, y_train_t_nn)\n",
    "                    elif algo == 'rf':\n",
    "                        tuned_rf.fit(X_train_t_nn_temp, y_train_t_nn)\n",
    "                        bk = fs.SelectFromModel(tuned_rf, prefit=True, max_features=k)\n",
    "\n",
    "                    X_train_t_nn_temp = bk.transform(X_train_t_nn)\n",
    "                    X_val_nn_temp = bk.transform(X_val_nn_temp)\n",
    "\n",
    "                tuner = kt.Hyperband(model_builder_2,\n",
    "                                     objective=kt.Objective(\"val_auc\", direction=\"max\"),\n",
    "                                     max_epochs=10,\n",
    "                                     factor=3,\n",
    "                                     seed=SEED, \n",
    "                                     directory=None,\n",
    "                                     project_name=\"odor\",\n",
    "                                     overwrite=True,)\n",
    "\n",
    "                tuner.search(X_train_t_nn_temp, \n",
    "                             y_train_t_nn, \n",
    "                             epochs=30,\n",
    "                             validation_data=(X_val_nn_temp, y_val_nn),\n",
    "                             callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\")])\n",
    "\n",
    "                # Get the optimal hyperparameters\n",
    "                best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "                model_temp = tuner.hypermodel.build(best_hps)\n",
    "                model_temp.summary()\n",
    "\n",
    "                history = model_temp.fit(X_train_t_nn_temp, \n",
    "                                     y_train_t_nn, \n",
    "                                     epochs=30, \n",
    "                                     validation_data=(X_val_nn_temp, y_val_nn), \n",
    "                                     verbose=0)\n",
    "\n",
    "                y_pred_keras = model_temp.predict(X_val_nn_temp).ravel()\n",
    "                auc_keras = metrics.roc_auc_score(y_val_nn, y_pred_keras)\n",
    "                accuracy_2.loc[k, algo] = auc_keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
